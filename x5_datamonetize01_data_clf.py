# -*- coding: utf-8 -*-
"""X5_DataMonetize01_data_clf.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1asqBj8h4pdToakYXogNZzzNRo1NWSiAD
"""

import pandas as pd
import datetime
import numpy as np
import random
import re
import scipy
import collections
import matplotlib.pyplot as plt
# %matplotlib inline

from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier

"""Подход к решению задачи по прогнозированию покупок беременных. Тестовое задание к хакатону от Х5. Испольнитель Бочаров А.М. скайп bam271074

Прогнозирование детских покупок можно сделать на основании покупок, которые женщина делает во время беременности. Известно, что в период беременности женщина нуждается в витаминах и фолиевой кислоте. Таким образом если женщина возраста от 16 до 40 лет часто покупает овощи, фрукты, овес, постное мясо, молочные изделия. На основании данной информации можно построить модель, обучить ее выявлять данную закономерность и использовать в работе.
"""

pip list

"""### Версии:

- `numpy`: 1.18.4
- `pandas`: 1.0.3
- `sklearn`: 0.21.3

# загрузим сгенерированный датасет и обучим классификатор
"""

#let s fix random state
value = 42
random.seed(value)
np.random.seed(value)

#let s upload file with dataset pokupty20000.csv
from google.colab import files
uploaded = files.upload()
for fn in uploaded.keys():
  print ('User uploaded file {name} with length {length} bytes'.format(name=fn, \
                                                                       length=len(uploaded[fn])))

data_df=pd.read_csv('pokupty20000.csv')
data_df.head()

X=data_df.Products
y=data_df.Target
X.head()

data_df.Target.value_counts().plot(kind='bar')

TOKEN_RE = re.compile(r'[\w\d]+')  #regular expression to start with

def tokenize_text_simple_regex(txt, min_token_size=2):
    """ This func tokenize text with TOKEN_RE applied ealier """
    txt = txt.lower()
    all_tokens = TOKEN_RE.findall(txt)
    return [token for token in all_tokens if len(token) >= min_token_size]

def tokenize_corpus(texts, tokenizer=tokenize_text_simple_regex, **tokenizer_kwargs):
    """
    This func tokenize corpus of docs
    """
    return [tokenizer(text, **tokenizer_kwargs) for text in texts]

text_tokenized = tokenize_corpus(X)

print(text_tokenized)

def build_vocabulary(tokenized_texts, max_size=100_000, max_doc_freq=1.0, min_count=1, pad_word=None):
    """ This func builds vocabulary """
    word_counts = collections.defaultdict(int)
    doc_n = 0

    # посчитать количество документов, в которых употребляется каждое слово
    # а также общее количество документов
    for txt in tokenized_texts:
        doc_n += 1
        unique_text_tokens = set(txt)
        for token in unique_text_tokens:
            word_counts[token] += 1

    # убрать слишком редкие и слишком частые слова
    word_counts = {word: cnt for word, cnt in word_counts.items()
                   if cnt >= min_count and cnt / doc_n <= max_doc_freq}

    # отсортировать слова по убыванию частоты
    sorted_word_counts = sorted(word_counts.items(),
                                reverse=True,
                                key=lambda pair: pair[1])

    # добавим несуществующее слово с индексом 0 для удобства пакетной обработки
    if pad_word is not None:
        sorted_word_counts = [(pad_word, 0)] + sorted_word_counts

    # если у нас по прежнему слишком много слов, оставить только max_size самых частотных
    if len(word_counts) > max_size:
        sorted_word_counts = sorted_word_counts[:max_size]

    # нумеруем слова
    word2id = {word: i for i, (word, _) in enumerate(sorted_word_counts)}

    # нормируем частоты слов
    word2freq = np.array([cnt / doc_n for _, cnt in sorted_word_counts], dtype='float32')

    return word2id, word2freq

# строим словарь
vocabulary, word_doc_freq = build_vocabulary(text_tokenized, max_doc_freq=1.0, min_count=1)
print("Размер словаря", len(vocabulary))
print(list(vocabulary.items())[:20])

def vectorize_texts(tokenized_texts, word2id, word2freq, mode='tfidf', scale=True):
    """ This func vectorize texts """
    assert mode in {'ltfidf', 'tfidf', 'idf', 'tf', 'bin'}
    # ltfidf is ln(TF+1)*IDF
    # считаем количество употреблений каждого слова в каждом документе
    result = scipy.sparse.dok_matrix((len(tokenized_texts), len(word2id)), dtype='float32')
    for text_i, text in enumerate(tokenized_texts):
        for token in text:
            if token in word2id:
                result[text_i, word2id[token]] += 1  #add 1 to element of matrix

    # получаем бинарные вектора "встречается или нет"
    if mode == 'bin':
        result = (result > 0).astype('float32')

    # получаем вектора относительных частот слова в документе
    elif mode == 'tf':
        result = result.tocsr()
        result = result.multiply(1 / result.sum(1))
    # учитываем всю информацию, которая у нас есть:
    # частоту слова в документе и частоту слова в корпусе
    elif mode == 'tfidf':
        result = result.tocsr()
        result = result.multiply(1 / result.sum(1))  # разделить каждую строку на её длину
        result = result.multiply(1 / word2freq)  # разделить каждый столбец на вес слова
    elif mode == 'ltfidf':
        result = result.tocsr()
        result = result.multiply(1 / result.sim(1))  # divide string to its len
        one_array = np.ones(len(tokenized_texts), len(word2id))
        result = np.log(result + one_array)
        result = result.multiply( 1 / word2freq)

    if scale:
        #result = result.tocsc()
        #result = result.std(0, ddof = 1)
        result = result.tocsc()
        result -= result.min()
        result /= (result.max() + 1e-6)

    return result.tocsr()

VECTORIZATION_MODE = 'tfidf'
train_vectors = vectorize_texts(text_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)

print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)

clf = GradientBoostingClassifier(random_state=value)
clf.fit(train_vectors, y)

print(train_vectors[:1])

"""1-onion  3-milk   7-fries"""

clf.predict(train_vectors[:1])

"""в выборке есть молоко, и классификатор правильно это определил"""

clf.score(train_vectors, y)